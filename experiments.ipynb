{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikel\\Documents\\GitHub\\multimodal-rag\\.venv\\Lib\\site-packages\\deepeval\\__init__.py:54: UserWarning: You are using deepeval version 2.7.1, however version 2.7.2 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from preprocessing_utils import load_all_files, preprocess_documents\n",
    "from pathlib import Path\n",
    "import os\n",
    "from api import GOOGLE_API_KEY\n",
    "import google.generativeai as genai\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_path = Path(\"papers/selection\")\n",
    "docs = load_all_files(papers_path)\n",
    "docs = preprocess_documents(docs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use around three sentences for your answer and keep it concise.\n",
    "Do not ever mention that you are using the context as source.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer:\"    \n",
    "\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain import hub\n",
    "# from langchain_chroma import Chroma\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import models\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "GENERATIVE_MODEL_NAME = \"gemini-2.0-flash-lite\"\n",
    "EMBEDDINGS_MODEL_NAME = \"models/text-embedding-004\"\n",
    "\n",
    "qdrant_client = QdrantClient(path=\"papers_db\")\n",
    "#### UNCOMMENT IF CREATING COLLECTION FROM SCRATCH\n",
    "# qdrant_client.create_collection(\n",
    "#     collection_name=\"selection\",\n",
    "#     vectors_config=models.VectorParams(\n",
    "#         size=768,\n",
    "#         distance=models.Distance.COSINE\n",
    "#     )\n",
    "\n",
    "# )\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=EMBEDDINGS_MODEL_NAME)\n",
    "vectorstore = QdrantVectorStore(\n",
    "    client=qdrant_client,\n",
    "    collection_name=\"selection\",\n",
    "    embedding=embeddings,\n",
    "    distance=models.Distance.COSINE\n",
    ")\n",
    "#### UNCOMMENT IF CREATING COLLECTION FROM SCRATCH\n",
    "# vectorstore.add_documents(docs)\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={\"k\": 3}\n",
    ")\n",
    "llm = ChatGoogleGenerativeAI(model=GENERATIVE_MODEL_NAME)\n",
    "prompt = custom_rag_prompt\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    \"\"\"Concatenates doc contents into a single string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    docs : list\n",
    "        list of Document objects\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        doc page_contents concatenated into a string\n",
    "    \"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks.\"\n",
    "    \"Use the following pieces of retrieved context to answer the question.\"\n",
    "    \"If you don't know the answer, just say that you don't know.\"\n",
    "    \"Use around three sentences for your answer and keep it concise.\"\n",
    "    \"Use LaTeX to format any math equation.\"\n",
    "    \"Do not ever mention that you are using the context as source.\"\n",
    "    \"Here is context for your answer:\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': 'iText® 5.3.5 ©2000-2012 1T3XT BVBA (SPRINGER SBM; licensed version)', 'creator': 'Springer', 'creationdate': '2021-08-31T00:12:39+05:30', 'keywords': '', 'crossmarkdomains[1]': 'springer.com', 'moddate': '2021-08-31T07:48:08+02:00', 'subject': 'Communications Materials, doi:10.1038/s43246-021-00194-3', 'doi': '10.1038/s43246-021-00194-3', 'author': 'Jiucheng Cheng', 'crossmarkdomains[2]': 'springerlink.com', 'title': 'A geometric-information-enhanced crystal graph network for predicting properties of materials', 'source': 'papers/selection/Cheng et al. - 2021 - A geometric-information-enhanced crystal graph net.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', '_id': '36825661a76a4aec975a0d94df1f33c1', '_collection_name': 'selection'}, page_content='aggregation process. In material prediction domain, the geometrical Crystal graph deﬁ nition and the introduction of geometric\\nstructure information like spatial distance and direction is alsoinformation. In this section, we construct a crystal graph\\nimportant because the relative spatial position of atoms in therepresentation suitable for any stoichiometric crystalline material.\\nmicrostructure is closely relatedto the charge interaction between23 Many Such a graph retains the information of the topological and\\nthem and thus affects macroscopic properties of the material. geometric structure of crystals. It also records the periodicity and\\nworks have already introduced geometric structure information into the key crystal information, such as the crystal lattice vector and\\ntheir models. The Crystal Graph Convolutional Neural Network the cell volume. Besides, representations in the graph meet'), Document(metadata={'producer': 'iText® 5.3.5 ©2000-2012 1T3XT BVBA (SPRINGER SBM; licensed version)', 'creator': 'Springer', 'creationdate': '2021-08-31T00:12:39+05:30', 'keywords': '', 'crossmarkdomains[1]': 'springer.com', 'moddate': '2021-08-31T07:48:08+02:00', 'subject': 'Communications Materials, doi:10.1038/s43246-021-00194-3', 'doi': '10.1038/s43246-021-00194-3', 'author': 'Jiucheng Cheng', 'crossmarkdomains[2]': 'springerlink.com', 'title': 'A geometric-information-enhanced crystal graph network for predicting properties of materials', 'source': 'papers/selection/Cheng et al. - 2021 - A geometric-information-enhanced crystal graph net.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', '_id': 'fca2da5956ac4f38adbff542c528c3bc', '_collection_name': 'selection'}, page_content='the hypothesis that each atom exists in an inﬁ nite deep spherical well. addition,werecordthelatticevector a; b; c and cell volume Ω as Pc\\nThe utilization of attention masks is an excellent way to encode the which means crystal parameters and they will be used in section C.\\ngeometrical structure among atoms. Since it is just beginning of the The full picture of the crystal graph is shown in Fig.1.\\nuse in the ﬁ eld of crystal material prediction, there is still room for\\ndevelopment. In this work, we propose a GNN model to accurately v0 ðz i Þ¼ W ðOnehotðz i ÞÞ; uij ¼ r!\\npredict properties for any crystalline materials, which is invariant to i ¼ Embedding ij ð1Þ\\nglobal 3D rotations, translations, and node permutations. Our model\\nachieves unprecedented predictionaccuracy by introducing complete\\nlocal spatial geometrical information. On the one hand, we construct A MPNN-based GNN for crystal property prediction'), Document(metadata={'producer': 'iText® 5.3.5 ©2000-2012 1T3XT BVBA (SPRINGER SBM; licensed version)', 'creator': 'Springer', 'creationdate': '2021-08-31T00:12:39+05:30', 'keywords': '', 'crossmarkdomains[1]': 'springer.com', 'moddate': '2021-08-31T07:48:08+02:00', 'subject': 'Communications Materials, doi:10.1038/s43246-021-00194-3', 'doi': '10.1038/s43246-021-00194-3', 'author': 'Jiucheng Cheng', 'crossmarkdomains[2]': 'springerlink.com', 'title': 'A geometric-information-enhanced crystal graph network for predicting properties of materials', 'source': 'papers/selection/Cheng et al. - 2021 - A geometric-information-enhanced crystal graph net.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', '_id': 'c6b420d73b04422b850137fdbbfa51b1', '_collection_name': 'selection'}, page_content='their models. The Crystal Graph Convolutional Neural Network the cell volume. Besides, representations in the graph meet\\n(CGCNN)19 chose the distance between atoms to represent the edges translation invariance and node permutation invariance.\\nin the crystal graph. The Materials Graph Network (MEGNet)24 In a molecule graph representation, nodes usually represent atoms\\nintroduced manual features that included topological distance and inthemoleculeandedgesrepresentthechemicalbondsbetween\\nspatial distance. The directional message passing neural network25 encoded the directional information into GNN modelsatoms18,31,32. But in crystals, there are no clearly deﬁ ned chemical\\n(DimeNet) bonds among atoms. Hence, it is necessary to deﬁ ne the adjacency\\nfor molecular materials26,27. However, previous GNN based works relationship among atoms ﬁ rst. Similar to CGCNN19,wedeﬁ ne the'), Document(metadata={'producer': 'iText® 5.3.5 ©2000-2012 1T3XT BVBA (SPRINGER SBM; licensed version)', 'creator': 'Springer', 'creationdate': '2021-08-31T00:12:39+05:30', 'keywords': '', 'crossmarkdomains[1]': 'springer.com', 'moddate': '2021-08-31T07:48:08+02:00', 'subject': 'Communications Materials, doi:10.1038/s43246-021-00194-3', 'doi': '10.1038/s43246-021-00194-3', 'author': 'Jiucheng Cheng', 'crossmarkdomains[2]': 'springerlink.com', 'title': 'A geometric-information-enhanced crystal graph network for predicting properties of materials', 'source': 'papers/selection/Cheng et al. - 2021 - A geometric-information-enhanced crystal graph net.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', '_id': '7cb51d1cf6ba44109eb58c704bcc17f4', '_collection_name': 'selection'}, page_content='Jiucheng Cheng1,4, Chunkai Zhang 1,4 ✉ & Lifeng Dong 2,3 ✉\\n\\n\\n\\n\\nGraph neural networks (GNNs) have been used previously for identifying new crystalline\\nmaterials. However, geometric structure is not usually taken into consideration, or only\\npartially. Here, we develop a geometric-information-enhanced crystal graph neural network\\n(GeoCGNN) to predict the properties of crystalline materials. By considering the distance\\nvector between each node and its neighbors, our model can learn full topological and spatial\\ngeometric structure information. Furthermore, we incorporate an effective method based on\\nthe mixed basis functions to encode the geometric information into our model, which out-\\nperforms other GNN methods in a variety of databases. For example, for predicting formation\\nenergy our model is 25.6%, 14.3% and 35.7% more accurate than CGCNN, MEGNet and\\niCGCNN models, respectively. For band gap, our model outperforms CGCNN by 27.6% and\\nMEGNet by 12.4%.')]\n",
      "One example is the Crystal Graph Convolutional Neural Network (CGCNN). It uses the distance between atoms to represent the edges in the crystal graph.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\":\n",
    "\"Give me an example architecture of a graph neural network used to predict physical properties of crystals and describe the internal architecture.\"\n",
    "                             })\n",
    "print(response[\"context\"])\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing_utils import load_queries, load_answers\n",
    "\n",
    "queries = load_queries(\"papers/selection_queries.txt\")\n",
    "answers = load_answers(\"papers/selection_answers.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1_helical_weyl.pdf',\n",
       " '2_magic_angle.pdf',\n",
       " '3_chiral_kagome.pdf',\n",
       " '4_quantum_supremacy.pdf',\n",
       " '5_wte2.pdf',\n",
       " '6_fgt.pdf',\n",
       " '7_kagome_order.pdf',\n",
       " '8_triangulene.pdf',\n",
       " '9_neutron_activation.pdf',\n",
       " '10_rucl3.pdf',\n",
       " '11_methylammonium.pdf',\n",
       " '12_perovskites.pdf',\n",
       " '13_high_troughput.pdf',\n",
       " '14_quenched_nematic.pdf',\n",
       " '15_fractional_charges.pdf',\n",
       " '16_plasmons.pdf',\n",
       " '17_charge_transfer.pdf',\n",
       " '18_photonic_axion.pdf',\n",
       " '19_adsorption.pdf',\n",
       " '20_cgnn.pdf']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_names = sorted(os.listdir(\"papers/selection\"), key=lambda x: int(x.split(\"_\")[0]))\n",
    "paper_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(2.1309297535714578)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from metrics import compute_dcg\n",
    "import numpy as np\n",
    "ideal_dcg = compute_dcg([\"a\", \"a\", \"a\"], np.array([1,1,1]), \"a\")\n",
    "ideal_dcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.90325211, 0.8043681 , 0.8931873 , 0.77793567, 0.90191179,\n",
       "       0.85582628, 0.73910535, 0.87486064, 0.87178229, 0.76854813,\n",
       "       0.75890711, 0.78719071, 0.66802446, 0.67354001, 0.83462882,\n",
       "       0.63187465, 0.80785569, 0.87585578, 0.29583043, 0.32937405,\n",
       "       0.63691264, 0.8279142 , 0.81288415, 0.3173956 , 0.73844581,\n",
       "       0.73137116, 0.764208  , 0.87312454, 0.82169079, 0.87638703,\n",
       "       0.84600412, 0.89798883, 0.79308244, 0.83783709, 0.80531714,\n",
       "       0.75374734, 0.71420121, 0.        , 0.26868167, 0.8270814 ,\n",
       "       0.82533922, 0.85486909, 0.91052406, 0.7836308 , 0.82682394,\n",
       "       0.80297807, 0.8536215 , 0.81635612, 0.88188953, 0.68063833,\n",
       "       0.89962569, 0.80594209, 0.82717863, 0.47147366, 0.80714767,\n",
       "       0.7986099 , 0.86533019, 0.77911123, 0.85701513, 0.72357773])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = []\n",
    "for questions, gold_name in zip(queries, paper_names):\n",
    "    for query in questions:\n",
    "        result = vectorstore.similarity_search_with_score(query)\n",
    "        retrieved_docs = [x[0] for x in result]\n",
    "        retrieved_scores = np.array([x[1] for x in result])\n",
    "        retrieved_names = [Path(doc.metadata[\"source\"]).name for doc in retrieved_docs]\n",
    "        scores.append(compute_dcg(retrieved_names, retrieved_scores, gold_name) / ideal_dcg)\n",
    "scores = np.array(scores)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10_rucl3.pdf': 12,\n",
       " '11_methylammonium.pdf': 7,\n",
       " '12_perovskites.pdf': 13,\n",
       " '13_high_troughput.pdf': 9,\n",
       " '14_quenched_nematic.pdf': 7,\n",
       " '15_fractional_charges.pdf': 20,\n",
       " '16_plasmons.pdf': 14,\n",
       " '17_charge_transfer.pdf': 11,\n",
       " '18_photonic_axion.pdf': 12,\n",
       " '19_adsorption.pdf': 10,\n",
       " '1_helical_weyl.pdf': 10,\n",
       " '20_cgnn.pdf': 11,\n",
       " '2_magic_angle.pdf': 13,\n",
       " '3_chiral_kagome.pdf': 19,\n",
       " '4_quantum_supremacy.pdf': 7,\n",
       " '5_wte2.pdf': 6,\n",
       " '6_fgt.pdf': 10,\n",
       " '7_kagome_order.pdf': 7,\n",
       " '8_triangulene.pdf': 6,\n",
       " '9_neutron_activation.pdf': 8}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_max_page = {}\n",
    "for doc in docs:\n",
    "    doc_max_page[Path(doc.metadata[\"source\"]).name] = doc.metadata[\"total_pages\"]\n",
    "\n",
    "doc_max_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrel = {}\n",
    "query_index = 1\n",
    "\n",
    "for group_index, questions in enumerate(queries):\n",
    "    for query in questions:\n",
    "        query_dict = qrel.setdefault(f\"q{query_index}\", {})\n",
    "        query_index += 1\n",
    "        for doc_index, doc_name in enumerate(paper_names):\n",
    "            for page_num in range(doc_max_page[doc_name]):\n",
    "                query_dict[f\"{doc_name}_{page_num + 1}\"] = 1 if group_index == doc_index else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = {}\n",
    "query_index = 1\n",
    "for questions, gold_name in zip(queries, paper_names):\n",
    "    for query in questions:\n",
    "        query_dict = run.setdefault(f\"q{query_index}\", {})\n",
    "        query_index += 1\n",
    "        result = vectorstore.similarity_search_with_score(query, k=1e6)\n",
    "        for point, score in result:\n",
    "            doc_name = Path(point.metadata[\"source\"]).name\n",
    "            page_num = point.metadata[\"page\"]\n",
    "            query_dict[f\"{doc_name}_{page_num+1}\"] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json.dump(run, open(\"run_text.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=GENERATIVE_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multimodal_rag.rag_model import QdrantMultiModalRAGModel\n",
    "from multimodal_rag.vlm import VLM\n",
    "\n",
    "VLM_MODEL = \"vidore/colqwen2.5-v0.2\"\n",
    "\n",
    "vlm = VLM.from_pretrained(\n",
    "    VLM_MODEL,\n",
    "    device=\"cuda:0\",\n",
    "    pool_factor=0\n",
    ")\n",
    "\n",
    "IMAGE_FOLDER = Path(\"images/selection\")\n",
    "\n",
    "rag = QdrantMultiModalRAGModel(vlm, qdrant_client, save_docs_path=IMAGE_FOLDER)\n",
    "rag.create_collection(\"selection\")\n",
    "rag.collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.save_from_folder(\"papers/selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9w/3l6rxjcs3bgfl3v7b2f66llr0000gn/T/ipykernel_77878/2457562682.py:44: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm(formatted_prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image features a tabby cat wearing orange sunglasses. The cat has green eyes and a white chest. It's sitting on what appears to be a sofa, with various cushions visible in the background. The image is well-lit and has a shallow depth of field, focusing attention on the cat's face.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate \n",
    "import base64\n",
    "from langchain_core.runnables import RunnableLambda \n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
    "\n",
    "top_k = 3\n",
    "\n",
    "# Folder for page images.\n",
    "IMAGE_FOLDER = Path(\"images/selection\")\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        return base64.b64encode(img_file.read()).decode(\"utf-8\")\n",
    "\n",
    "def load_image_from_point(point):\n",
    "    image_path = IMAGE_FOLDER / f\"{point.id}.jpg\"\n",
    "    return encode_image(image_path)\n",
    "\n",
    "def retrieve_points(query, limit=10, top_k=3):\n",
    "    result = rag.search(\n",
    "        query,\n",
    "        limit=limit\n",
    "    )\n",
    "    return result.points[:top_k]\n",
    "\n",
    "def retrieve_images(query, limit=10, top_k=3):\n",
    "    result = rag.search(\n",
    "        query,\n",
    "        limit=limit\n",
    "    )\n",
    "\n",
    "    images = []\n",
    "    for point in result.points[:top_k]:\n",
    "        images.append(load_image_from_point(point))\n",
    "\n",
    "    return images\n",
    "\n",
    "def expand_context_images(message):\n",
    "    new_message = {}\n",
    "    new_message[\"query\"] = message[\"query\"]\n",
    "    for i, image in enumerate(message[\"context_images\"]):\n",
    "        new_message[f\"context{i}\"] = image\n",
    "    \n",
    "    return new_message\n",
    "\n",
    "system_message = (\n",
    "    \"You are an assistant for question-answering tasks.\"\n",
    "    \"Use the images of retrieved context to answer the question.\"\n",
    "    \"If you don't know the answer, just say that you don't know.\"\n",
    "    \"Use around three sentences for your answer and keep it concise.\"\n",
    "    \"Use LaTeX to format any math equation.\"\n",
    "    \"Do not ever mention that you are using the context as source.\"\n",
    ") \n",
    "\n",
    "user_message = [\n",
    "    {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": \"{query}\",\n",
    "    }\n",
    "] + [\n",
    "    {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": {\"url\": \"data:image/jpeg;base64,{placeholder}\".replace(\"placeholder\", f\"context{i}\")},\n",
    "    }\n",
    "    for i in range(top_k)\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_message),\n",
    "        (\n",
    "            \"user\",\n",
    "            user_message,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "rag_chain = (\n",
    "    {\"context_images\": retrieve_images, \"query\": RunnablePassthrough()}\n",
    "    | RunnableLambda(expand_context_images)\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context0', 'context1', 'query'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks.Use the images of retrieved context to answer the question.If you don't know the answer, just say that you don't know.Use around three sentences for your answer and keep it concise.Do not ever mention that you are using the context as source.\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=[PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), ImagePromptTemplate(input_variables=['context0'], input_types={}, partial_variables={}, template={'url': 'data:image/jpeg;base64,{context0}'}), ImagePromptTemplate(input_variables=['context1'], input_types={}, partial_variables={}, template={'url': 'data:image/jpeg;base64,{context1}'})], additional_kwargs={})])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "def load_docs_as_dict(root_path, mode=\"page\"):\n",
    "    \"\"\"Loads PDF documents into a dictionary indexed by their name where\n",
    "    each value is a list of Document objects, one for each page\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    root_path : str | Path\n",
    "        path where the PDF documents are\n",
    "    mode : str, optional\n",
    "        how to chunk the documents, by default \"page\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        document dictionary\n",
    "    \"\"\"\n",
    "    docs = {}\n",
    "    num_files = 0\n",
    "    for path, folders, files in os.walk(root_path):\n",
    "        path = Path(path)\n",
    "        if len(folders) != 0:\n",
    "            continue\n",
    "        for file in files:\n",
    "            if not file.endswith(\".pdf\"):\n",
    "                continue\n",
    "            doc_loader = PyPDFLoader(path / file, mode=mode, extraction_mode=\"layout\")\n",
    "            pages = preprocess_documents(doc_loader.load())\n",
    "            docs.setdefault(file, pages)\n",
    "            num_files += 1\n",
    "            print(f\"Number of files: {num_files}\")\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_dict = load_docs_as_dict(Path(\"papers/selection\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=GENERATIVE_MODEL_NAME)\n",
    "\n",
    "top_k = 3\n",
    "\n",
    "def retrieve_text(query, limit=10, top_k=3):\n",
    "    result = rag.search(\n",
    "        query,\n",
    "        limit=limit\n",
    "    )\n",
    "\n",
    "    doc_text = []\n",
    "    for point in result.points[:top_k]:\n",
    "        doc_name = point.payload[\"doc_name\"]\n",
    "        page_num = point.payload[\"page_num\"]\n",
    "        doc_text.append(docs_dict[doc_name][page_num - 1].page_content)\n",
    "\n",
    "    return \"\\n\\n\".join(doc_text)\n",
    "\n",
    "template = (\n",
    "    \"You are an assistant for question-answering tasks.\"\n",
    "    \"Use the following pieces of retrieved context to answer the question.\"\n",
    "    \"If you don't know the answer, just say that you don't know.\"\n",
    "    \"Use around three sentences for your answer and keep it concise.\"\n",
    "    \"Use LaTeX to format any math equation.\"\n",
    "    \"Do not ever mention that you are using the context as source.\"\n",
    "    \"Here is the question:\\n\"\n",
    "    \"{query}\\n\"\n",
    "    \"Here is context for your answer:\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    "    \"\\n\\n\"\n",
    "    \"Answer:\"\n",
    ") \n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "     {\"context\": retrieve_text, \"query\": RunnablePassthrough()}\n",
    "     | prompt\n",
    "     | llm\n",
    "     | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing_utils import load_queries, load_answers\n",
    "\n",
    "queries = load_queries(\"papers/selection_queries.txt\")\n",
    "answers = load_answers(\"papers/selection_answers.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `trec_eval` gold and generated items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_names = sorted(os.listdir(\"papers/selection\"), key=lambda x: int(x.split(\"_\")[0]))\n",
    "\n",
    "# Get max number of pages per document\n",
    "doc_max_page = {}\n",
    "for record in rag.list_items(limit=1000):\n",
    "    doc_max_page.setdefault(record.payload[\"doc_name\"], 1)\n",
    "    page_num = record.payload[\"page_num\"]\n",
    "    if page_num > doc_max_page[record.payload[\"doc_name\"]]:\n",
    "        doc_max_page[record.payload[\"doc_name\"]] = page_num\n",
    "doc_max_page\n",
    "\n",
    "# qrel holds goldens for retrieval\n",
    "qrel = {}\n",
    "query_index = 1\n",
    "\n",
    "for group_index, questions in enumerate(queries):\n",
    "    for query in questions:\n",
    "        query_dict = qrel.setdefault(f\"q{query_index}\", {})\n",
    "        query_index += 1\n",
    "        for doc_index, doc_name in enumerate(paper_names):\n",
    "            for page_num in range(doc_max_page[doc_name]):\n",
    "                query_dict[f\"{doc_name}_{page_num + 1}\"] = 1 if group_index == doc_index else 0\n",
    "\n",
    "query_index = 1\n",
    "# run holds pipeline retrievals\n",
    "run = {}\n",
    "for questions, gold_name in zip(queries, paper_names):\n",
    "    for query in questions:\n",
    "        # use VLM retriever or text retriever\n",
    "        result = rag.search(query, limit=1e6)\n",
    "        query_dict = run.setdefault(f\"q{query_index}\", {})\n",
    "        query_index += 1\n",
    "        for point in result.points:\n",
    "            doc_name = point.payload[\"doc_name\"]\n",
    "            page_num = point.payload[\"page_num\"]\n",
    "            query_dict[f\"{doc_name}_{page_num}\"] = point.score\n",
    "\n",
    "run     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytrec_eval import RelevanceEvaluator\n",
    "import json\n",
    "\n",
    "evaluator = RelevanceEvaluator(qrel, {\"ndcg\", \"ndcg_cut_3\", \"ndcg_cut_5\"})\n",
    "evaluation = evaluator.evaluate(run)\n",
    "json.dump(evaluation, open(\"ndcg_vlm.json\", 'w'), indent=4)\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sleeping after 15 queries\n",
      "Resuming\n",
      "Sleeping after 15 queries\n",
      "Resuming\n",
      "Sleeping after 15 queries\n",
      "Resuming\n",
      "Sleeping after 15 queries\n",
      "Resuming\n"
     ]
    }
   ],
   "source": [
    "from deepeval.dataset import EvaluationDataset\n",
    "from preprocessing_utils import create_dataset\n",
    "\n",
    "\n",
    "dataset = create_dataset(rag_chain, queries, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation dataset saved at papers/selection_gemini-2.0-flash-lite_text\\20250413_122238.json!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'papers/selection_gemini-2.0-flash-lite_text\\\\20250413_122238.json'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.save_as(file_type=\"json\", directory=\"papers/selection_gemini-2.0-flash-lite_text\", include_test_cases=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "DATASET_PATH = Path(r\"papers\\selection_gemini-2.0-flash-lite_vlm\\20250413_104009.json\")\n",
    "dataset = EvaluationDataset()\n",
    "dataset.add_test_cases_from_json_file(\n",
    "    file_path=DATASET_PATH,\n",
    "    input_key_name=\"input\",\n",
    "    actual_output_key_name=\"actual_output\",\n",
    "    expected_output_key_name=\"expected_output\",\n",
    "    context_key_name=\"context\",\n",
    "    retrieval_context_key_name=\"retrieval_context\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.models.llms import GeminiModel\n",
    "from deepeval import evaluate\n",
    "from time import sleep\n",
    "\n",
    "judge_llm = GeminiModel(\n",
    "    model_name=\"gemini-2.0-flash-lite\",\n",
    "    api_key=GOOGLE_API_KEY\n",
    ")\n",
    "\n",
    "answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5, model=judge_llm, include_reason=True)\n",
    "\n",
    "# Evaluate by batches to not saturate Google Generative AI free quota\n",
    "\n",
    "results = []\n",
    "batch_size = 5\n",
    "test_case_index = 0\n",
    "number_errors = 0\n",
    "for i, test_case in enumerate(dataset.test_cases):\n",
    "    try:\n",
    "        result = evaluate(test_cases=[test_case], metrics=[answer_relevancy_metric])\n",
    "    except AttributeError:\n",
    "        print(\"Error in test case:\", i)\n",
    "        print(\"Continuing...\")\n",
    "        number_errors += 1\n",
    "    results.append(result)\n",
    "    if (i + 1) % batch_size == 0:\n",
    "        print(\"Processed\", i + 1, \" test cases\")\n",
    "        sleep(65)\n",
    "\n",
    "print(number_errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(results, open(DATASET_PATH.parent / \"answer_relevancy.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:29<00:00,  2.06it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "EMBEDDINGS_MODEL_NAME = \"models/text-embedding-004\"\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=EMBEDDINGS_MODEL_NAME)\n",
    "\n",
    "actual_embeddings = []\n",
    "expected_embeddings = []\n",
    "for test_case in tqdm(dataset.test_cases):\n",
    "    actual_vector = embeddings.embed_query(test_case.actual_output)\n",
    "    actual_embeddings.append(actual_vector)\n",
    "    expected_vector = embeddings.embed_query(test_case.expected_output)\n",
    "    expected_embeddings.append(expected_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from metrics import similarity_scores\n",
    "\n",
    "scores = similarity_scores(actual_embeddings, expected_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intepreting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "names = [\"text\", \"vlm\", \"mixed\", \"text-lite\", \"vlm-lite\", \"mixed-lite\"]\n",
    "paths = [\n",
    "    \"papers/results/selection_gemini-2.0-flash_text\",\n",
    "    \"papers/results/selection_gemini-2.0-flash_vlm\",\n",
    "    \"papers/results/selection_gemini-2.0-flash_mixed\",\n",
    "    \"papers/results/selection_gemini-2.0-flash-lite_text\",\n",
    "    \"papers/results/selection_gemini-2.0-flash-lite_vlm\",\n",
    "    \"papers/results/selection_gemini-2.0-flash-lite_mixed\",\n",
    "]\n",
    "\n",
    "scores = {}\n",
    "for name, path in zip(names, paths):\n",
    "    scores.setdefault(name, np.loadtxt(Path(path) / \"similarity_scores.dat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name : text Avg. score : 0.7661053808199424 Std : 0.11677131030116435\n",
      "Name : vlm Avg. score : 0.7635383269990966 Std : 0.1202173267946917\n",
      "Name : mixed Avg. score : 0.7696908588926106 Std : 0.0989059049784914\n",
      "Name : text-lite Avg. score : 0.7671149432196328 Std : 0.10751080514355006\n",
      "Name : vlm-lite Avg. score : 0.7703999576004855 Std : 0.08909902159490196\n",
      "Name : mixed-lite Avg. score : 0.775280121309187 Std : 0.09620779013599447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x12ba5bd10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for name in names:\n",
    "    print(f\"Name : {name} Avg. score : {scores[name].mean()} Std : {scores[name].std()}\")\n",
    "    plt.hist(scores[name], bins=20, alpha=0.5, label=name)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Explain what kind of control operations were used in the quantum supremacy circuits.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(queries).ravel()[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([59,  7,  8, 57, 34, 48, 49, 31, 54, 50, 28, 41, 40, 51, 53, 29, 13,\n",
       "       17, 39, 27, 26, 42,  5, 18, 45, 32, 47, 11, 19, 25, 30, 43, 37, 10,\n",
       "        4,  0, 36, 23, 44, 56, 33, 22, 12, 16, 24, 15, 35, 20,  3, 21, 52,\n",
       "        2,  9,  6, 55, 14, 58,  1, 38, 46])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(scores[\"text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "physics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
