{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing_utils import load_all_files, preprocess_documents\n",
    "from pathlib import Path\n",
    "import os\n",
    "from api import GOOGLE_API_KEY\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n",
      "Rotated text discovered. Output will be incomplete.\n"
     ]
    }
   ],
   "source": [
    "papers_path = Path(\"papers/selection\")\n",
    "docs = load_all_files(papers_path)\n",
    "docs = preprocess_documents(docs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use around three sentences for your answer and keep it concise.\n",
    "Do not ever mention that you are using the context as source.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer:\"    \n",
    "\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain import hub\n",
    "# from langchain_chroma import Chroma\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import models\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "# splits = text_splitter.split_documents(docs)\n",
    "qdrant_client = QdrantClient(path=\"papers_db\")\n",
    "# client.create_collection(\n",
    "#     collection_name=\"selection\",\n",
    "#     vectors_config=models.VectorParams(\n",
    "#         size=768,\n",
    "#         distance=models.Distance.COSINE\n",
    "#     )\n",
    "\n",
    "# )\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "# vectorstore = Chroma.from_documents(\n",
    "#     documents=splits,\n",
    "#     embedding=embeddings,\n",
    "#     persist_directory=\"selection_db\"\n",
    "#  )\n",
    "# vectorstore = Chroma(\n",
    "#     persist_directory=\"selection_db\",\n",
    "#     embedding_function=embeddings\n",
    "# )\n",
    "vectorstore = QdrantVectorStore(\n",
    "    client=qdrant_client,\n",
    "    collection_name=\"selection\",\n",
    "    embedding=embeddings,\n",
    "    distance=models.Distance.COSINE\n",
    ")\n",
    "# vectorstore.add_documents(splits)\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "prompt = custom_rag_prompt\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# rag_chain = (\n",
    "#     {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "#     | prompt\n",
    "#     | llm\n",
    "#     | StrOutputParser()\n",
    "# )\n",
    "\n",
    "# rag_chain.invoke(\"What is Task Decomposition?\")\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks.\"\n",
    "    \"Use the following pieces of retrieved context to answer the question.\"\n",
    "    \"If you don't know the answer, just say that you don't know.\"\n",
    "    \"Keep your answer concise and to the point.\"\n",
    "    \"Use LaTeX for any math equation.\"\n",
    "    # \"Use around three sentences for your answer and keep it concise.\"\n",
    "    \"Do not ever mention that you are using the context as source.\"\n",
    "    \"Here is context for your answer:\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "# rag_chain.invoke({\"input\": \"What kind of material is MAPbI3?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': 'iText¬Æ 5.3.5 ¬©2000-2012 1T3XT BVBA (SPRINGER SBM; licensed version)', 'creator': 'Springer', 'creationdate': '2021-08-31T00:12:39+05:30', 'keywords': '', 'crossmarkdomains[1]': 'springer.com', 'moddate': '2021-08-31T07:48:08+02:00', 'subject': 'Communications Materials, doi:10.1038/s43246-021-00194-3', 'doi': '10.1038/s43246-021-00194-3', 'author': 'Jiucheng Cheng', 'crossmarkdomains[2]': 'springerlink.com', 'title': 'A geometric-information-enhanced crystal graph network for predicting properties of materials', 'source': 'papers\\\\selection\\\\Cheng et al. - 2021 - A geometric-information-enhanced crystal graph net.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', '_id': '64a0bfbb3d48419a91198657aff450eb', '_collection_name': 'selection'}, page_content='aggregation process. In material prediction domain, the geometrical Crystal graph deÔ¨Å nition and the introduction of geometric\\nstructure information like spatial distance and direction is alsoinformation. In this section, we construct a crystal graph\\nimportant because the relative spatial position of atoms in therepresentation suitable for any stoichiometric crystalline material.\\nmicrostructure is closely relatedto the charge interaction between23 Many Such a graph retains the information of the topological and\\nthem and thus affects macroscopic properties of the material. geometric structure of crystals. It also records the periodicity and\\nworks have already introduced geometric structure information into the key crystal information, such as the crystal lattice vector and\\ntheir models. The Crystal Graph Convolutional Neural Network the cell volume. Besides, representations in the graph meet'), Document(metadata={'producer': 'iText¬Æ 5.3.5 ¬©2000-2012 1T3XT BVBA (SPRINGER SBM; licensed version)', 'creator': 'Springer', 'creationdate': '2021-08-31T00:12:39+05:30', 'keywords': '', 'crossmarkdomains[1]': 'springer.com', 'moddate': '2021-08-31T07:48:08+02:00', 'subject': 'Communications Materials, doi:10.1038/s43246-021-00194-3', 'doi': '10.1038/s43246-021-00194-3', 'author': 'Jiucheng Cheng', 'crossmarkdomains[2]': 'springerlink.com', 'title': 'A geometric-information-enhanced crystal graph network for predicting properties of materials', 'source': 'papers\\\\selection\\\\Cheng et al. - 2021 - A geometric-information-enhanced crystal graph net.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', '_id': '49c024241428465eb2d1aeb36cde531b', '_collection_name': 'selection'}, page_content='the hypothesis that each atom exists in an inÔ¨Å nite deep spherical well. addition,werecordthelatticevector a; b; c and cell volume Œ© as Pc\\nThe utilization of attention masks is an excellent way to encode the which means crystal parameters and they will be used in section C.\\ngeometrical structure among atoms. Since it is just beginning of the The full picture of the crystal graph is shown in Fig.1.\\nuse in the Ô¨Å eld of crystal material prediction, there is still room for\\ndevelopment. In this work, we propose a GNN model to accurately v0 √∞z i √û¬º W √∞Onehot√∞z i √û√û; uij ¬º r!\\npredict properties for any crystalline materials, which is invariant to i ¬º Embedding ij √∞1√û\\nglobal 3D rotations, translations, and node permutations. Our model\\nachieves unprecedented predictionaccuracy by introducing complete\\nlocal spatial geometrical information. On the one hand, we construct A MPNN-based GNN for crystal property prediction'), Document(metadata={'producer': 'iText¬Æ 5.3.5 ¬©2000-2012 1T3XT BVBA (SPRINGER SBM; licensed version)', 'creator': 'Springer', 'creationdate': '2021-08-31T00:12:39+05:30', 'keywords': '', 'crossmarkdomains[1]': 'springer.com', 'moddate': '2021-08-31T07:48:08+02:00', 'subject': 'Communications Materials, doi:10.1038/s43246-021-00194-3', 'doi': '10.1038/s43246-021-00194-3', 'author': 'Jiucheng Cheng', 'crossmarkdomains[2]': 'springerlink.com', 'title': 'A geometric-information-enhanced crystal graph network for predicting properties of materials', 'source': 'papers\\\\selection\\\\Cheng et al. - 2021 - A geometric-information-enhanced crystal graph net.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', '_id': '6654cb1c5b5f45ce9cd66da6fcbf7634', '_collection_name': 'selection'}, page_content='their models. The Crystal Graph Convolutional Neural Network the cell volume. Besides, representations in the graph meet\\n(CGCNN)19 chose the distance between atoms to represent the edges translation invariance and node permutation invariance.\\nin the crystal graph. The Materials Graph Network (MEGNet)24 In a molecule graph representation, nodes usually represent atoms\\nintroduced manual features that included topological distance and inthemoleculeandedgesrepresentthechemicalbondsbetween\\nspatial distance. The directional message passing neural network25 encoded the directional information into GNN modelsatoms18,31,32. But in crystals, there are no clearly deÔ¨Å ned chemical\\n(DimeNet) bonds among atoms. Hence, it is necessary to deÔ¨Å ne the adjacency\\nfor molecular materials26,27. However, previous GNN based works relationship among atoms Ô¨Å rst. Similar to CGCNN19,wedeÔ¨Å ne the'), Document(metadata={'producer': 'iText¬Æ 5.3.5 ¬©2000-2012 1T3XT BVBA (SPRINGER SBM; licensed version)', 'creator': 'Springer', 'creationdate': '2021-08-31T00:12:39+05:30', 'keywords': '', 'crossmarkdomains[1]': 'springer.com', 'moddate': '2021-08-31T07:48:08+02:00', 'subject': 'Communications Materials, doi:10.1038/s43246-021-00194-3', 'doi': '10.1038/s43246-021-00194-3', 'author': 'Jiucheng Cheng', 'crossmarkdomains[2]': 'springerlink.com', 'title': 'A geometric-information-enhanced crystal graph network for predicting properties of materials', 'source': 'papers\\\\selection\\\\Cheng et al. - 2021 - A geometric-information-enhanced crystal graph net.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', '_id': 'b05dc078e06944208d3577fa38b8ba9d', '_collection_name': 'selection'}, page_content='Jiucheng Cheng1,4, Chunkai Zhang 1,4 ‚úâ & Lifeng Dong 2,3 ‚úâ\\n\\n\\n\\n\\nGraph neural networks (GNNs) have been used previously for identifying new crystalline\\nmaterials. However, geometric structure is not usually taken into consideration, or only\\npartially. Here, we develop a geometric-information-enhanced crystal graph neural network\\n(GeoCGNN) to predict the properties of crystalline materials. By considering the distance\\nvector between each node and its neighbors, our model can learn full topological and spatial\\ngeometric structure information. Furthermore, we incorporate an effective method based on\\nthe mixed basis functions to encode the geometric information into our model, which out-\\nperforms other GNN methods in a variety of databases. For example, for predicting formation\\nenergy our model is 25.6%, 14.3% and 35.7% more accurate than CGCNN, MEGNet and\\niCGCNN models, respectively. For band gap, our model outperforms CGCNN by 27.6% and\\nMEGNet by 12.4%.')]\n",
      "One example of a graph neural network architecture used to predict the physical properties of crystals is the Crystal Graph Convolutional Neural Network (CGCNN). In this architecture, the distance between atoms is used to represent the edges in the crystal graph. The nodes represent atoms. The graph retains the information of the topological and geometric structure of crystals and records the periodicity and key crystal information, such as the crystal lattice vector and the cell volume.\n",
      "Another example is the geometric-information-enhanced crystal graph neural network (GeoCGNN), which considers the distance vector between each node and its neighbors to learn full topological and spatial geometric structure information.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\":\n",
    "\"Give me an example architecture of a graph neural network used to predict physical properties of crystals and describe the internal architecture.\"\n",
    "                             })\n",
    "print(response[\"context\"])\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9w/3l6rxjcs3bgfl3v7b2f66llr0000gn/T/ipykernel_77878/2457562682.py:44: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm(formatted_prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image features a tabby cat wearing orange sunglasses. The cat has green eyes and a white chest. It's sitting on what appears to be a sofa, with various cushions visible in the background. The image is well-lit and has a shallow depth of field, focusing attention on the cat's face.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "import base64\n",
    "\n",
    "# Function to encode an image as base64\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        return base64.b64encode(img_file.read()).decode(\"utf-8\")\n",
    "\n",
    "# Encode the image(s)\n",
    "image_path = \"image.jpg\"  # Replace with your image path\n",
    "image_base64 = encode_image(image_path)\n",
    "\n",
    "# Define system message (instructions for the AI)\n",
    "system_template = \"You are an AI assistant that analyzes images and answers user queries.\"\n",
    "system_message = SystemMessagePromptTemplate.from_template(system_template)\n",
    "\n",
    "# Define human message (text + image context)\n",
    "human_template = \"{text}\"  # Placeholder for user input text\n",
    "human_message = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "# Create the chat prompt template\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "\n",
    "# Format the chat prompt with user input\n",
    "formatted_prompt = chat_prompt.format_messages(\n",
    "    text=\"What can you tell me about this image?\",\n",
    ")\n",
    "\n",
    "# Append image to the human message dynamically\n",
    "formatted_prompt.append(\n",
    "    HumanMessage(\n",
    "        content=[\n",
    "            {\"type\": \"text\", \"text\": \"Here is the image context:\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": f\"data:image/jpeg;base64,{image_base64}\"}\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Initialize Gemini model (multimodal)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "\n",
    "# Get the response from the model\n",
    "response = llm(formatted_prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The images show a cat wearing orange sunglasses. Yes, they are the same.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "top_k = 2\n",
    "\n",
    "def retrieve_images(query):\n",
    "    return [encode_image(\"image.jpg\") for i in range(top_k)]\n",
    "\n",
    "def print_thing(thing):\n",
    "    print(thing)\n",
    "    return thing\n",
    "\n",
    "def expand_context_images(message):\n",
    "    new_message = {}\n",
    "    new_message[\"query\"] = message[\"query\"]\n",
    "    for i, image in enumerate(message[\"context_images\"]):\n",
    "        new_message[f\"context{i}\"] = image\n",
    "    \n",
    "    return new_message\n",
    "\n",
    "system_message = (\n",
    "    \"You are an assistant for question-answering tasks.\"\n",
    "    \"Use the images of retrieved context to answer the question.\"\n",
    "    \"If you don't know the answer, just say that you don't know.\"\n",
    "    \"Use around three sentences for your answer and keep it concise.\"\n",
    "    \"Do not ever mention that you are using the context as source.\"\n",
    ") \n",
    "\n",
    "user_message = [\n",
    "    {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": \"{query}\",\n",
    "    }\n",
    "] + [\n",
    "    {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": {\"url\": \"data:image/jpeg;base64,{placeholder}\".replace(\"placeholder\", f\"context{i}\")},\n",
    "    }\n",
    "    for i in range(top_k)\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_message),\n",
    "        (\n",
    "            \"user\",\n",
    "            user_message,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\", system_message),\n",
    "#         (\n",
    "#             \"user\",\n",
    "#             [\n",
    "#                 {\n",
    "#                     \"type\": \"text\",\n",
    "#                     \"text\": \"{query}\",\n",
    "\n",
    "#                 },\n",
    "#                 {\n",
    "#                     \"type\": \"image_url\",\n",
    "#                     \"image_url\": {\"url\": \"data:image/jpeg;base64,\"},\n",
    "#                 }\n",
    "#             ],\n",
    "#         ),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context_images\": retrieve_images, \"query\": RunnablePassthrough()}\n",
    "    | RunnableLambda(expand_context_images)\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What are these images? Are they the same?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context0', 'context1', 'query'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks.Use the images of retrieved context to answer the question.If you don't know the answer, just say that you don't know.Use around three sentences for your answer and keep it concise.Do not ever mention that you are using the context as source.\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=[PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), ImagePromptTemplate(input_variables=['context0'], input_types={}, partial_variables={}, template={'url': 'data:image/jpeg;base64,{context0}'}), ImagePromptTemplate(input_variables=['context1'], input_types={}, partial_variables={}, template={'url': 'data:image/jpeg;base64,{context1}'})], additional_kwargs={})])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this block for Gemini Developer API\n",
    "from google import genai\n",
    "genai_client = genai.Client(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing_utils import load_queries, load_answers\n",
    "\n",
    "queries = load_queries(\"papers/selection_queries.txt\")\n",
    "answers = load_answers(\"papers/selection_answers.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gemini-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">2.0</span><span style=\"color: #374151; text-decoration-color: #374151\">-flash, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gemini-\u001b[0m\u001b[1;38;2;55;65;81m2.0\u001b[0m\u001b[38;2;55;65;81m-flash, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|100% (1/1) [Time Taken: 00:01,  2.00s/test case]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚ùå Answer Relevancy (score: 0.0, threshold: 0.7, strict: False, evaluation model: gemini-2.0-flash, reason: The score is 0.00 because the statement in the actual output is completely irrelevant to the input question about shoe fit., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What if these shoes don't fit?\n",
      "  - actual output: I like trains.\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 0.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Tests finished üéâ! Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval login'</span> to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "‚ú®üëÄ Looking for a place for your LLM test data to live üè°‚ù§Ô∏è ? Use <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span> to get &amp; share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run <span style=\"color: #008080; text-decoration-color: #008080\">'deepeval login'</span> in the CLI. \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Tests finished üéâ! Run \u001b[1;32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "‚ú®üëÄ Looking for a place for your LLM test data to live üè°‚ù§Ô∏è ? Use \u001b[38;2;106;0;255mConfident AI\u001b[0m to get & share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run \u001b[36m'deepeval login'\u001b[0m in the CLI. \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval_utils import DeepevalGeminiModel\n",
    "\n",
    "# Replace this with the actual output from your LLM application\n",
    "actual_output = \"I like trains.\"\n",
    "llm_judge = DeepevalGeminiModel(genai_client, model_name=\"gemini-2.0-flash\")\n",
    "\n",
    "metric = AnswerRelevancyMetric(\n",
    "    threshold=0.7,\n",
    "    model=llm_judge,\n",
    "    include_reason=True\n",
    ")\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What if these shoes don't fit?\",\n",
    "    actual_output=actual_output\n",
    ")\n",
    "\n",
    "# To run metric as a standalone\n",
    "# metric.measure(test_case)\n",
    "# print(metric.score, metric.reason)\n",
    "\n",
    "result = evaluate(test_cases=[test_case], metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.dataset import EvaluationDataset, Golden\n",
    "\n",
    "test_cases = []\n",
    "for doc_queries, doc_answers in zip(queries, answers):\n",
    "    for query, answer in zip(doc_queries, doc_answers):\n",
    "        test_case = LLMTestCase(\n",
    "            input=query,\n",
    "            expected_output=answer,\n",
    "            actual_output=rag_chain.invoke({\"input\": query})[\"answer\"],\n",
    "        )\n",
    "        test_cases.append(test_case)\n",
    "dataset = EvaluationDataset(test_cases=test_cases)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5, model=llm_judge, include_reason=True)\n",
    "\n",
    "dataset.evaluate([answer_relevancy_metric])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
